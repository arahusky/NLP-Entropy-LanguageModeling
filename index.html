<!DOCTYPE html>
<html lang="en-US">

<head>
<title>NLP Homework - Jakub Naplava</title>
<meta name="keywords" content="NLP">
<meta name="author" content="Jakub Naplava">
<meta charset="UTF-8">

<style>
.table-header-rotated th.row-header{
  width: auto;
}

.table-header-rotated td{
  width: 40px;
  border-top: 1px solid #dddddd;
  border-left: 1px solid #dddddd;
  border-right: 1px solid #dddddd;
  vertical-align: middle;
  text-align: center;
}

.table-header-rotated th.column criterion rotate-45{
  height: 80px;
  width: 40px;
  min-width: 40px;
  max-width: 40px;
  position: relative;
  vertical-align: bottom;
  padding: 0;
  font-size: 12px;
  line-height: 0.8;
}

.table-header-rotated th.column criterion rotate-45 > div{
  position: relative;
  top: 0px;
  left: 40px; /* 80 * tan(45) / 2 = 40 where 80 is the height on the cell and 45 is the transform angle*/
  height: 100%;
  -ms-transform:skew(-45deg,0deg);
  -moz-transform:skew(-45deg,0deg);
  -webkit-transform:skew(-45deg,0deg);
  -o-transform:skew(-45deg,0deg);
  transform:skew(-45deg,0deg);
  overflow: hidden;
  border-left: 1px solid #dddddd;
  border-right: 1px solid #dddddd;
  border-top: 1px solid #dddddd;
}

.table-header-rotated th.column criterion rotate-45 span {
  -ms-transform:skew(45deg,0deg) rotate(315deg);
  -moz-transform:skew(45deg,0deg) rotate(315deg);
  -webkit-transform:skew(45deg,0deg) rotate(315deg);
  -o-transform:skew(45deg,0deg) rotate(315deg);
  transform:skew(45deg,0deg) rotate(315deg);
  position: absolute;
  bottom: 30px; /* 40 cos(45) = 28 with an additional 2px margin*/
  left: -25px; /*Because it looked good, but there is probably a mathematical link here as well*/
  display: inline-block;
  width: 85px; /* 80 / cos(45) - 40 cos (45) = 85 where 80 is the height of the cell, 40 the width of the cell and 45 the transform angle*/
  text-align: left;
  /* white-space: nowrap; *//*whether to display in one line or not*/
}

</style>

<script src='https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script>


</head>

<body>

<h1> Homework NLP </h1> 

<p> All code was written in Java (there is also Netbeans project attached) and expects both <i>TEXTEN1.txt</i> and <i>TEXTCZ1.txt</i> to be in the main folder. Also note that this html file uses <a target="_blank" href="https://www.mathjax.org">MathJax</a> to display math, so it is highly recommended to have an internet connection while loading this page.</p>  

<h2>First task - Entropy</h2>
<p> The first task was to compute conditional (bigram) entropy of given texts. The code for the programming part of this task can be found in <a href="src/homework/Homework1.java">Homework1.java</a>. </p>

<p> The <a href="entropy.html">complete list of entropy measurements</a> is because of readability reasons omitted on this page. Let's discuss here results of the experiment.  </p>

<figure>
  <img src="image/entropy/im00.png">
  <figcaption>Fig1. - Entropy of the texts with respect to certain messup.</figcaption>
</figure>

<p> Figure1 shows the measured entropy with respect to certain messup probabilities. We can clearly see that with higher character messup, the entropy of the text decreases. The reason for this is that when we change some character in a word, the new word will probably be a nonsense word with no current occurrences in the text. This means that the number of unique words in the text increases with the higher probability of character messup. With more unique words, the bigram probabilities P(X|Y) will be higher (closer to non-uniform) and the the conditional (bigram) entropy of the new text will be lower than the old one. We can even consider extreme messup probabily of 1.0. In this case, each character is changed; therefore, most words in the new text will be unique and the conditional entropy would be close to 0. </p>

<p> Figure1 also shows that with higher word messup probability, the entropy of the English text grows, whereas the entropy of the Czech text decreases. This behaviour is much more difficult to me to understand, because it is not clear, what should the change of one word to another do with entropy of the text.</p>

<figure>
  <table border="1"> 
  	<thead>
		<th>Text</th>
		<th>Entropy</th>
		<th>Number of words</th>
		<th>Avg. word length</th>
		<th>Number of unique words</th>		
		<th>Words used 1x</th> 
		<th>Words used 2x</th> 
		<th>Words used 3x</th> 
  	</thead>
	<tr>
		<td><b>English</b></td>
		<td>5.287</td>
		<td>221098</td>
		<td>4.40</td>
		<td>9607</td>		
		<td>3811</td>
		<td>1371</td>
		<td>743</td>
	</tr>
	<tr>
		<td><b>Czech</b></td>
		<td>4.748</td>
		<td>222412</td>
		<td>4.63</td>
		<td>42826</td>	
		<td>26315</td>
		<td>6592</td>
		<td>2888</td>	
	</tr>
  </table>
  <figcaption>Table1. - English vs Czech text comparison.</figcaption>
</figure>

<p> Table1 shows comparison of the English and the Czech text. Both texts have very similar length and also the average word length, but the Czech text has much higher amount of unique and infrequent words. I mainly suspect this fact to be a reason of the slightly lower entropy of the Czech text compared to the Englist text. The reason, why more unique (as well as infrequent) words in the text leads to its lower entropy, was already explained above.</p>

<h3> Paper-and-pencil exercise ("Conditional entropy of merged text") </h3>

 <p>We have two languages \(L_1\) and \(L_2\) with no shared vocabulary items. The conditional (bigram) entropy of a text \(T_1\)/\(T_2\) in language \(L_1\)/\(L_2\) is E (=same). The task is to decide, what will be the conditional entropy of text \(T_1\).\(T_2\) (appending \(T_2\) to the end of \(T_1\)) like. </p>

 <p> Let us denote \(c_1(x,y)\) the number of occurences of word x followed by word y in the text \(T_1\) (normally denoted as c(x,y)), \(c_1(x)\) number of occurences of word x in the text \(T_1\) (normally denoted as c(x)) and similarly \(c_2(x,y)\) and \(c_2(x)\) for the text \(T_2\). Then from the previous pararagraph we know that:
 	 $$ E = - \sum\limits_{x,y \in T_1} P(x,y) log_2 P(y|x) = - \sum\limits_{x,y \in T_1} \frac{c_1(x,y)}{|T_1|} log_2 P(y|x) = - \frac{1}{|T_1|} \sum\limits_{x,y \in T_1} c_1(x,y) log_2 P(y|x) = - \frac{1}{|T_1|} \sum\limits_{x,y \in T_1} c_1(x,y) log_2 \frac{c_1(x,y)}{c_1(x)} $$
 	 $$ E |T_1| = - \sum\limits_{x,y \in T_1} c_1(x,y) log_2 \frac{c_1(x,y)}{c_1(x)} $$
	 $$ similarly\ for\ T_2: E |T_2| = - \sum\limits_{x,y \in T_2} c_2(x,y) log_2 \frac{c_2(x,y)}{c_2(x)} $$
</p>

<p>
	The unknown entropy \(E_{new}\) of the new text T is defined as:
	$$ E_{new} = - \frac{1}{|T_1| + |T_2|} \sum\limits_{x,y \in T} c(x,y) log_2 \frac{c(x,y)}{c(x)} $$
</p>

<p>
	The new text T consists of words from \(T_1\) and \(T_2\) and because words in \(T_1\) are different from words in \(T_2\) (and vica versa), the counts c(x) and c(x,y) will be following:
	
	$$ c(x) = \begin{cases} c_1(x) &\mbox{if } x \in T_1	 \\ 
c_2(x) & \mbox{if } x \in T_2 
\end{cases} $$

	$$ c(x,y) = \begin{cases} 1 &\mbox{if } x\ is\ the\ last\ word\ in\ T_1\ (=w)\ and\ y\ is\ the\ first\ word\ in\ T_2	 \\ 
c_1(x,y) & \mbox{if } x,y \in T_1 \\
c_2(x,y) & \mbox{if } x,y \in T_2 \\
0 & \mbox{otherwise }
\end{cases} $$
</p>

<p> 
	From this notion, we can rewrite \(E_{new}\):

	$$ E_{new} = - \frac{1}{|T_1| + |T_2|} \sum\limits_{x,y \in T} c(x,y) log_2 \frac{c(x,y)}{c(x)} = - \frac{1}{|T_1| + |T_2|} (\sum\limits_{x,y \in T_1} c_1(x,y) log_2 \frac{c_1(x,y)}{c_1(x)} + \sum\limits_{x,y \in T_2} c_2(x,y) log_2 \frac{c_2(x,y)}{c_2(x)} + 1*log_2 \frac{1}{c_1(w)}) = \\
	= - \frac{1}{|T_1| + |T_2|} (-E|T_1| -E|T_2| + log_2 \frac{1}{c_1(w)}) = E + \frac{log_2 (c_1(w))}{|T_1| + |T_2|}
	$$
</p>
<p>
	Finally, since word w occurs in the text at least once, the entropy of the new next \(E_{new}\) is either same as the entropy of texts \(T_1\) and \(T_2\) (if word w is unique in \(T_1\)) or bigger (if w occurs multiple times in \(T_1\)). 
</p>

<h2>Second task - Cross entropy and language modeling</h2>
<p> The second task was to program EM smoothing algorithm, run it with given texts (over the heldout data), compute the cross-entropy of the test data using the built smoothed language model and finally tweak model parameters and compute the cross-entropy again. The code for the programming part of this task can be found in <a href="src/homework/Homework2.java">Homework2.java</a>. </p>

<p> As already noted, the heldout data must be used for estimation of the smoothed language model parameters (lambdas). When we try to run the smoothing algorithm with the train data, we will always get lambda[3] (almost - depends on the selected epsilon) equal to 1.  </p>


<figure>
  <table border="1"> 
  	<thead>
		<th>Text</th>
		<th>[λ0,λ1,λ2,λ3]</th>
		<th>Cross-entropy</th>
  	</thead>
	<tr>
		<td><b>English</b></td>
		<td>[0.0702, 0.2538, 0.4922, 0.1838]</td>
		<td>7.4685</td>
	</tr>
	<tr>
		<td><b>Czech</b></td>
		<td>[0.1405, 0.4289, 0.2450, 0.1860]</td>
		<td>10.2208</td>
	</tr>
  </table>
  <figcaption>Table2. - Lambdas and the cross-entropy of the smoothed models (epsilon=0.0001).</figcaption>
</figure>

<figure>
  <table border="1"> 
  	<thead>
		<th>Text</th>
		<th>Coverage (all test data)</th>
		<th>Coverage (unique words in test data)</th>
  	</thead>
	<tr>
		<td><b>English</b></td>
		<td>0.9556</td>
		<td>0.7582</td>
	</tr>
	<tr>
		<td><b>Czech</b></td>
		<td>0.8646</td>
		<td>0.6517</td>
	</tr>
  </table>
  <figcaption>Table3. - Coverage (percentage of words in the test data which have been seen in the training data) .</figcaption>
</figure>

<p>Table2 shows the computed parameters for the smoothed language models. There were 11 iterations of EM smoothing algorithm while computing lambdas for the English text and 17 iterations for the Czech text. We can see that the cross-entropy of the Czech text is now higher than the cross-entropy of the English text. This may be again caused by the difference in unique word counts together with difference in the 'coverage' shown in Table3. There we can see that the coverage for the Czech text (counted for all as well as only for unique words) is lower than for the English text.</p>


<figure>
  <table border="1"> 
  	<thead>
		<th class="column criterion rotate-45">Text</th>
		<th class="column criterion rotate-45">λ3 = 0</th>
		<th class="column criterion rotate-45">λ3 *= 0.1</th>
		<th class="column criterion rotate-45">λ3 *= 0.2</th>
		<th class="column criterion rotate-45">λ3 *= 0.3</th>
		<th class="column criterion rotate-45">λ3 *= 0.4</th>
		<th class="column criterion rotate-45">λ3 *= 0.5</th>
		<th class="column criterion rotate-45">λ3 *= 0.6</th>
		<th class="column criterion rotate-45">λ3 *= 0.7</th>
		<th class="column criterion rotate-45">λ3 *= 0.8</th>
		<th class="column criterion rotate-45">λ3 *= 0.9</th>
		<th class="column criterion rotate-45">λ3 </th>
		<th class="column criterion rotate-45">λ3 += 0.10 * (1-λ3)</th>
		<th class="column criterion rotate-45">λ3 += 0.20 * (1-λ3)</th>
		<th class="column criterion rotate-45">λ3 += 0.30 * (1-λ3)</th>
		<th class="column criterion rotate-45">λ3 += 0.40 * (1-λ3)</th>
		<th class="column criterion rotate-45">λ3 += 0.50 * (1-λ3)</th>
		<th class="column criterion rotate-45">λ3 += 0.60 * (1-λ3)</th>
		<th class="column criterion rotate-45">λ3 += 0.70 * (1-λ3)</th>
		<th class="column criterion rotate-45">λ3 += 0.80 * (1-λ3)</th>
		<th class="column criterion rotate-45">λ3 += 0.90 * (1-λ3)</th>
		<th class="column criterion rotate-45">λ3 += 0.95 * (1-λ3)</th>
		<th class="column criterion rotate-45">λ3 += 0.99 * (1-λ3)</th>
  	</thead>
	<tr>
		<td><b>English</b></td>
		<td>7.70</td>
		<td>7.61</td>
		<td>7.57</td>
		<td>7.54</td>
		<td>7.52</td>
		<td>7.50</td>
		<td>7.49</td>
		<td>7.48</td>
		<td>7.47</td>
		<td>7.47</td>
		<td>7.46</td>
		<td>7.47</td>
		<td>7.49</td>
		<td>7.54</td>
		<td>7.62</td>
		<td>7.72</td>
		<td>7.86</td>
		<td>8.05</td>
		<td>8.34</td>
		<td>8.85</td>
		<td>9.36</td>
		<td>10.5</td>
	</tr>
	<tr>
		<td><b>Czech</b></td>
		<td>10.48</td>
		<td>10.36</td>
		<td>10.32</td>
		<td>10.29</td>
		<td>10.27</td>
		<td>10.25</td>
		<td>10.24</td>
		<td>10.23</td>
		<td>10.22</td>
		<td>10.22</td>
		<td>10.22</td>
		<td>10.22</td>
		<td>10.25</td>
		<td>10.30</td>
		<td>10.38</td>
		<td>10.48</td>
		<td>10.62</td>
		<td>10.81</td>
		<td>11.10</td>
		<td>11.60</td>
		<td>12.09</td>
		<td>13.16</td>
	</tr>
  </table>
  <figcaption>Table3. - Cross-entropy with respect to λ3 modifications.</figcaption>
</figure>

<figure>
  <img src="image/cross-entropy/im01.png">
  <figcaption>Fig2. - Cross-entropy with respect to λ3 modifications.</figcaption>
</figure>

<p>Table3 and Fig2 show the cross-entropy when modyfing the parameters. We can see that the minimal value of the cross-entropy is for both texts in the point with original parameters and the far from this point we go, the higher the cross-entropy is. The goal of the EM smoothing algorithm is to find parameters to make the cross-entropy lowest. Therefore, the more we try to change the parameters, the bigger should the cross-entropy be. We can also see that the cross-entropy changes much more when going right from the original lambda than if going left from it. This is in my opinion caused by the fact that the combination of bigram, unigram and uniform models holds enough information even for the test data, while the original trigram model self does not. </p>

</body>

</html>